{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming that the data is already normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the different sets \n",
    "training_set = []\n",
    "testing_set = []\n",
    "validation_set = []\n",
    "\n",
    "# Choosing to create a dataframe that stores the numpy array and the action name\n",
    "data = pd.DataFrame(columns= ['action_id', 'action', 'label'])\n",
    "#data_norm = pd.DataFrame(columns= ['action_id_norm', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_action_down = 'actions/action_down'\n",
    "path_action_up = 'actions/action_up'\n",
    "path_action_switch = 'actions/action_switch'\n",
    "path_arr = [path_action_down, path_action_up, path_action_switch]\n",
    "\n",
    "\n",
    "action_id = []\n",
    "action = []\n",
    "label_action = []\n",
    "action_id_norm = []\n",
    "\n",
    "# Normalise the data\n",
    "def min_max_normalise(arr):\n",
    "    min = np.min(arr)\n",
    "    #print(min)\n",
    "    max = np.max(arr)\n",
    "    #print(max)\n",
    "    normalise = (arr - min)/(max-max)\n",
    "    print(arr)\n",
    "    print(normalise)\n",
    "    return normalise\n",
    "\n",
    "# for action in the path, get name and array and add to the dataframe\n",
    "# Work from the dataset\n",
    "for path_type in path_arr:\n",
    "    for file in os.listdir(path_type):\n",
    "        f = os.path.join(path_type, file)\n",
    "        heatmap = np.load(f)\n",
    "\n",
    "        '''\n",
    "        #normalise\n",
    "        normalise = min_max_normalise(heatmap)\n",
    "        if not os.path.exists('normalised_data'):    \n",
    "            os.makedirs('normalised_data')\n",
    "        norm_path = os.path.join('normalised_data', file)\n",
    "        save_norm = np.save(norm_path, normalise)\n",
    "        '''\n",
    "\n",
    "        action_id.append(file)\n",
    "        action.append(path_type)\n",
    "\n",
    "        if path_type is path_arr[0]:\n",
    "            label = 'D'\n",
    "        if path_type is path_arr[1]:\n",
    "            label = 'U'\n",
    "        if path_type is path_arr[2]:\n",
    "            label = 'S'\n",
    "\n",
    "        label_action.append(label)\n",
    "        #action_id_norm.append(save_norm)\n",
    "\n",
    "data['action_id'] = action_id\n",
    "data['action'] = action\n",
    "data['label'] = label_action\n",
    "\n",
    "#data_norm['action_id_norm'] = action_id_norm\n",
    "#data_norm['action'] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_id</th>\n",
       "      <th>action</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>action_down_1610_0.npy</td>\n",
       "      <td>actions/action_down</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>action_down_1655_0.npy</td>\n",
       "      <td>actions/action_down</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>action_down_547_0.npy</td>\n",
       "      <td>actions/action_down</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>action_down_502_0.npy</td>\n",
       "      <td>actions/action_down</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>action_down_990_0.npy</td>\n",
       "      <td>actions/action_down</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                action_id               action label\n",
       "0  action_down_1610_0.npy  actions/action_down     D\n",
       "1  action_down_1655_0.npy  actions/action_down     D\n",
       "2   action_down_547_0.npy  actions/action_down     D\n",
       "3   action_down_502_0.npy  actions/action_down     D\n",
       "4   action_down_990_0.npy  actions/action_down     D"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7485, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['down', 'up', 'switch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7485\n",
      "7485\n"
     ]
    }
   ],
   "source": [
    "# array x is array of npy arra<ys and Y is array of labels\n",
    "X = []\n",
    "Y = label_action\n",
    "\n",
    "for path_type in path_arr:\n",
    "    for file in os.listdir(path_type):\n",
    "        f = os.path.join(path_type, file)\n",
    "        heatmap = np.load(f)\n",
    "        X.append(heatmap)\n",
    "\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5988\n",
      "1497\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5988, 17, 64, 48)\n"
     ]
    }
   ],
   "source": [
    "all_train = np.empty((5988, 17, 64, 48))\n",
    "for i in range(0, len(X_train)):\n",
    "    all_train[i] = X_train[i]\n",
    "print(all_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of CNN\n",
    "Convolution + ReLU --> Pooling --> Convolution + ReLU --> Pooling -->...--> Flatten --> Fully connected --> softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Operation (filter) and Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 1\n",
    "# Choose a filter\n",
    "conv1 = np.random.randn(1,17,4,4) * np.sqrt(1. / 5)\n",
    "filter_w = conv1.shape[3]\n",
    "filter_h = conv1.shape[2]\n",
    "# Create resulting matrix R\n",
    "image_w = 48\n",
    "image_h = 64\n",
    "result_w = (int) ((image_w-filter_w)/stride)+1\n",
    "result_h = (int) ((image_h-filter_h)/stride)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU - learn complex patterns\n",
    "def ReLU(x):\n",
    "    return (x>0)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5988, 17, 61, 45)\n"
     ]
    }
   ],
   "source": [
    "correlations_per_filter = np.empty((5988, 17, result_h, result_w))\n",
    "for image in X_train: # Change this to all data\n",
    "    for f in range(0, len(image[0])):\n",
    "        conv_filter = conv1[0][f]\n",
    "        image_filter = image[0][f]\n",
    "        correlation_valid = signal.correlate2d(image_filter, conv_filter, mode='valid')\n",
    "        activated = ReLU(correlation_valid)\n",
    "        correlations_per_filter[f] = activated\n",
    "\n",
    "print(correlations_per_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.20323510e-04,  1.05567382e-04,  6.57867487e-04, ...,\n",
       "         4.82129533e-03, -0.00000000e+00,  3.52744625e-04],\n",
       "       [ 1.33448774e-04,  3.87780086e-04,  2.76501404e-04, ...,\n",
       "         2.68109299e-03,  1.12663092e-02, -0.00000000e+00],\n",
       "       [-0.00000000e+00,  2.31607803e-04,  5.94882991e-04, ...,\n",
       "         2.95992649e-03,  1.69868343e-03,  7.68081595e-03],\n",
       "       ...,\n",
       "       [ 7.62618692e-04,  1.21867822e-04,  1.22213459e-04, ...,\n",
       "         1.22408927e-04,  1.43028246e-04,  1.10342333e-04],\n",
       "       [ 6.28075042e-04,  1.23760500e-04,  1.22018199e-04, ...,\n",
       "         1.19846287e-04,  9.81622508e-05,  1.25086298e-04],\n",
       "       [ 7.63139474e-04,  6.77027749e-04,  6.07475328e-05, ...,\n",
       "         1.03399769e-04, -0.00000000e+00,  1.84674453e-04]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_per_filter[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_two(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def perform_max_pool(single_correlation, w, h, pool_size, stride):\n",
    "    reshaped = tf.reshape(single_correlation, [1,h,w,17])\n",
    "    max_pool = tf.keras.layers.MaxPooling2D(pool_size, strides=stride, padding='valid')\n",
    "    return max_pool(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max pooling - Downsize feature map - extract lowlevel features \n",
    "# Experiments: Keras has maxpooling2d, averagepooling2d\n",
    "# Hyper parameter tuning - stride and pooling size\n",
    "max_pool_stride = 2\n",
    "pool_size = 2\n",
    "\n",
    "input_width = correlations_per_filter.shape[3]\n",
    "input_height = correlations_per_filter.shape[2]\n",
    "    \n",
    "output_width = int((input_width-pool_size)/max_pool_stride)+1\n",
    "output_height = int((input_height-pool_size)/max_pool_stride)+1\n",
    "    \n",
    "pools = np.zeros((correlations_per_filter.shape[0],output_height,output_width, correlations_per_filter.shape[1]))\n",
    "\n",
    "for i in range(0, correlations_per_filter.shape[0]):\n",
    "    pools[i] = perform_max_pool(correlations_per_filter[i], input_width, input_height, (2,2), (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 22, 17)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pools[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_per_filter = np.empty((5988, 17, result_h, result_w))\n",
    "\n",
    "def convolution_step(conv1, images):\n",
    "    for image in images: # Change this to all data\n",
    "        for f in range(0, len(image[0])):\n",
    "            conv_filter = conv1[0][f]\n",
    "            image_filter = image[0][f]\n",
    "            correlation_valid = signal.correlate2d(image_filter, conv_filter, mode='valid')\n",
    "            activated = ReLU(correlation_valid)\n",
    "            correlations_per_filter[f] = activated\n",
    "\n",
    "print(correlations_per_filter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation - Fully connected layers (Maybe watch video part as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class classification - Multiple neurons with a softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs - cycles of training the NN with all the training data\n",
    "# Pooling - reduce size of image\n",
    "# Classification - fully connected layer - dense layer with output for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
